{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset and test the performance with testing data\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling sklearn.tree.DecisionTreeClassifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data\n",
    "The dataset is the Heart Disease Data Set from UCI Machine Learning Repository. It is a binary classifiation dataset, the label is stored in `target` column. **Please note that there exist categorical features which need to be [one-hot encoding](https://www.datacamp.com/community/tutorials/categorical-data) before fit into your model!**\n",
    "See follow links for more information\n",
    "https://archive.ics.uci.edu/ml/datasets/heart+Disease"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "df = pd.read_csv(file_url)\n",
    "\n",
    "df_onehot = pd.get_dummies(df, columns=['thal'])\n",
    "\n",
    "train_idx = np.load('train_idx.npy')\n",
    "test_idx = np.load('test_idx.npy')\n",
    "\n",
    "x_train = df.iloc[train_idx]\n",
    "x_test = df.iloc[test_idx]\n",
    "\n",
    "train_df = np.array(df_onehot.iloc[train_idx])\n",
    "test_df = np.array(df_onehot.iloc[test_idx])\n",
    "\n",
    "y_train_df = train_df[:, 12]\n",
    "y_test_df = test_df[:, 12]\n",
    "\n",
    "train_df = np.concatenate((train_df[:, 0:11], train_df[:, 15:17]), axis=1)\n",
    "test_df = np.concatenate((test_df[:, 0:11], test_df[:, 15:17]), axis=1)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def gini(sequence):\n",
    "    sequence_1 = sequence[np.where(sequence == 1)]\n",
    "    sequence_2 = sequence[np.where(sequence == 2)]\n",
    "    p_1 = sequence_1.size / sequence.size\n",
    "    p_2 = sequence_2.size / sequence.size\n",
    "    return 1 - p_1 ** 2 - p_2 ** 2\n",
    "\n",
    "def entropy(sequence):\n",
    "    sequence_1 = sequence[np.where(sequence == 1)]\n",
    "    sequence_2 = sequence[np.where(sequence == 2)]\n",
    "    p_1 = sequence_1.size / sequence.size\n",
    "    p_2 = sequence_2.size / sequence.size\n",
    "    return -(p_1 * np.log2(p_1) + p_2 * np.log2(p_2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the test data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of test data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# The node used in decision tree\n",
    "class Node:\n",
    "    def __init__(self, gini, entropy, num_samples,\n",
    "                 num_samples_per_class, predicted_class):\n",
    "        self.gini = gini\n",
    "        self.entropy = entropy\n",
    "        self.num_samples = num_samples\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.feature_list = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classess = np.unique(y).size\n",
    "        self.features = X.shape[1]\n",
    "        self.tree = self.insert_tree(X, y)\n",
    "\n",
    "    def gini_(self, sequence):  # Calculate the Gini index\n",
    "        sequence_0 = sequence[np.where(sequence == 0)]\n",
    "        sequence_1 = sequence[np.where(sequence == 1)]\n",
    "        p_0 = sequence_0.size / sequence.size\n",
    "        p_1 = sequence_1.size / sequence.size\n",
    "        return 1 - p_0 ** 2 - p_1 ** 2\n",
    "\n",
    "    def entropy_(self, sequence):  # Calculate the entropy\n",
    "        sequence_0 = sequence[np.where(sequence == 0)]\n",
    "        sequence_1 = sequence[np.where(sequence == 1)]\n",
    "        p_0 = sequence_0.size / sequence.size\n",
    "        p_1 = sequence_1.size / sequence.size\n",
    "        return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n",
    "\n",
    "    # Find the best split\n",
    "    def best_split(self, X, y, criterion):\n",
    "        if y.size <= 1:\n",
    "            return None, None\n",
    "\n",
    "        best_gini = self.gini_(y)\n",
    "        best_entropy = self.entropy_(y)\n",
    "        thrsh, feature_idx = None, None\n",
    "\n",
    "        for i in range(X.shape[1]):  # number of features\n",
    "            feature_sorted = sorted(np.unique(X[:, i]))  # sort unique value\n",
    "            for j in range(len(feature_sorted)-1):  # number of unique values\n",
    "                # I first found the number of unique values and sorted them,\n",
    "                # and then calcualted the mean between two values as threshold\n",
    "                midpoint = (feature_sorted[j] + feature_sorted[j+1]) / 2\n",
    "                # If the value is less than threshold,\n",
    "                # it will belong to the left\n",
    "                left_X = X[np.where(X[:, i] < midpoint), :]\n",
    "                left_y = y[np.where(X[:, i] < midpoint)]\n",
    "                # If the value is greater than threshold,\n",
    "                # it will belong to the left\n",
    "                right_X = X[np.where(X[:, i] > midpoint), :]\n",
    "                right_y = y[np.where(X[:, i] > midpoint)]\n",
    "                if (criterion == 'gini'):\n",
    "                    gini_left = self.gini_(left_y)\n",
    "                    gini_right = self.gini_(right_y)\n",
    "                    gini_temp = (gini_left * left_y.size +\n",
    "                                 gini_right * right_y.size) / y.size\n",
    "                    if (gini_temp < best_gini):\n",
    "                        best_gini = gini_temp\n",
    "                        thrsh = midpoint\n",
    "                        # The i-th column of x is the feature\n",
    "                        # used to split the node\n",
    "                        feature_idx = i\n",
    "                else:\n",
    "                    entropy_left = self.entropy_(left_y)\n",
    "                    entropy_right = self.entropy_(right_y)\n",
    "                    entropy_temp = (entropy_left * left_y.size +\n",
    "                                    entropy_right * right_y.size) / y.size\n",
    "                    if (entropy_temp < best_entropy):\n",
    "                        best_entropy = entropy_temp\n",
    "                        thrsh = midpoint\n",
    "                        feature_idx = i\n",
    "\n",
    "        return feature_idx, thrsh\n",
    "\n",
    "    # Build the decision tree through recursive method\n",
    "    def insert_tree(self, X, y, depth=0):\n",
    "        num_class_0 = y[np.where(y == 0)].size\n",
    "        num_class_1 = y[np.where(y == 1)].size\n",
    "        num_samples_per_class = np.array([num_class_0, num_class_1])\n",
    "        # If the number of class 2 > class 1, then predict class 2\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "\n",
    "        node = Node(gini=self.gini_(y),\n",
    "                    entropy=self.entropy_(y),\n",
    "                    num_samples=y.size,\n",
    "                    num_samples_per_class=num_samples_per_class,\n",
    "                    predicted_class=predicted_class,\n",
    "                    )\n",
    "\n",
    "        if self.max_depth is not None:\n",
    "            if depth < self.max_depth:\n",
    "                feature, thrsh = self.best_split(X, y, self.criterion)\n",
    "                if feature is not None:\n",
    "                    feature_left = X[:, feature] < thrsh\n",
    "                    X_left, y_left = X[feature_left], y[feature_left]\n",
    "                    X_right, y_right = X[~feature_left], y[~feature_left]\n",
    "                    node.feature_index = feature\n",
    "                    self.feature_list.append(feature)\n",
    "                    node.threshold = thrsh\n",
    "                    node.left = self.insert_tree(X_left, y_left, depth + 1)\n",
    "                    node.right = self.insert_tree(X_right, y_right, depth + 1)\n",
    "        else:\n",
    "            feature, thrsh = self.best_split(X, y, self.criterion)\n",
    "            if feature is not None:\n",
    "                feature_left = X[:, feature] < thrsh\n",
    "                X_left, y_left = X[feature_left], y[feature_left]\n",
    "                X_right, y_right = X[~feature_left], y[~feature_left]\n",
    "                node.feature_index = feature\n",
    "                self.feature_list.append(feature)\n",
    "                node.threshold = thrsh\n",
    "                node.left = self.insert_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self.insert_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    # This function is used to record the feature used in every node\n",
    "    def feature_counter(self, X):\n",
    "        idx_list_counter = np.zeros(X.shape[1])\n",
    "        for i in range(len(self.feature_list)):\n",
    "            idx_list_counter[self.feature_list[i]] = \\\n",
    "                idx_list_counter[self.feature_list[i]] + 1\n",
    "\n",
    "        x = np.arange(len(idx_list_counter))\n",
    "        features = x_train.columns[np.where(idx_list_counter != 0)]\n",
    "        times = idx_list_counter[np.where(idx_list_counter != 0)]\n",
    "\n",
    "        plt.barh(features, times)\n",
    "        plt.ylabel('Features')\n",
    "        plt.xlabel('Counts')\n",
    "        plt.title(f'Feature importance\\n\\\n",
    "        criterion = {self.criterion}, max depth = {self.max_depth}')\n",
    "        plt.show()\n",
    "\n",
    "    # Predict the test data\n",
    "    def predict(self, test):\n",
    "        predicted = []\n",
    "        for data in test:\n",
    "            node = self.tree\n",
    "            while node.left:\n",
    "                if data[node.feature_index] <= node.threshold:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            predicted.append(node.predicted_class)\n",
    "        return predicted\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "clf_depth3 = DecisionTree(criterion = 'gini', max_depth = 3)\n",
    "clf_depth3.fit(train_df, y_train_df)\n",
    "pred_gini_depth3 = np.array(clf_depth3.predict(test_df))\n",
    "acc_gini_depth3 = accuracy_score(pred_gini_depth3, y_test_df)\n",
    "print(f\"criterion: gini, max_depth: 3, Accuracy = {acc_gini_depth3}\")\n",
    "\n",
    "\n",
    "clf_depth10 = DecisionTree(criterion = 'gini', max_depth = 10)\n",
    "clf_depth10.fit(train_df, y_train_df)\n",
    "pred_gini_depth10 = np.array(clf_depth10.predict(test_df))\n",
    "acc_gini_depth10 = accuracy_score(pred_gini_depth10, y_test_df)\n",
    "print(f\"criterion: gini, max_depth: 10, Accuracy = {acc_gini_depth10}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "criterion: gini, max_depth: 3, Accuracy = 0.83\n",
      "criterion: gini, max_depth: 10, Accuracy = 0.78\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: divide by zero encountered in log2\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n",
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n",
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: divide by zero encountered in log2\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n",
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of test data by `criterion=gini` and `criterion=entropy`, respectively."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth3.fit(train_df, y_train_df)\n",
    "pred_gini_depth3 = np.array(clf_depth3.predict(test_df))\n",
    "acc_gini_depth3 = accuracy_score(pred_gini_depth3, y_test_df)\n",
    "print(f\"criterion: gini, max_depth: 3, Accuracy = {acc_gini_depth3}\")\n",
    "\n",
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "clf_entropy.fit(train_df, y_train_df)\n",
    "pred_entropy_depth3 = np.array(clf_entropy.predict(test_df))\n",
    "acc_entropy_depth3 = accuracy_score(pred_entropy_depth3, y_test_df)\n",
    "print(f\"criterion: entropy, max_depth: 3, Accuracy = {acc_entropy_depth3}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "criterion: gini, max_depth: 3, Accuracy = 0.83\n",
      "criterion: entropy, max_depth: 3, Accuracy = 0.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: divide by zero encountered in log2\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n",
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n",
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: divide by zero encountered in log2\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n",
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Note: Your decisition tree scores should over **0.7**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "clf_depth10.feature_counter(train_df)\n",
    "# The feature which is not in the figure is not used"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Decision stump used as weak classifier\n",
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        self.polarity = 1\n",
    "        self.feature_idx = None\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        X_column = X[:, self.feature_idx]\n",
    "        predictions = np.ones(n_samples)\n",
    "        if self.polarity == 1:\n",
    "            predictions[X_column < self.threshold] = -1\n",
    "        else:\n",
    "            predictions[X_column > self.threshold] = -1\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class Adaboost():\n",
    "\n",
    "    def __init__(self, n_estimators=100):\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights to 1/N\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "        self.clfs = []\n",
    "        # Iterate through classifiers\n",
    "        for _ in range(self.n_estimators):\n",
    "            clf = DecisionStump()\n",
    "\n",
    "            min_error = float('inf')\n",
    "            # greedy search to find best threshold and feature\n",
    "            for feature_i in range(n_features):\n",
    "                X_column = X[:, feature_i]\n",
    "                thresholds = np.unique(X_column)\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    # predict with polarity 1\n",
    "                    p = 1\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    predictions[X_column < threshold] = -1\n",
    "\n",
    "                    # Error = sum of weights of misclassified samples\n",
    "                    misclassified = w[y != predictions]\n",
    "                    error = sum(misclassified)\n",
    "\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "\n",
    "                    # store the best configuration\n",
    "                    if error < min_error:\n",
    "                        clf.polarity = p\n",
    "                        clf.threshold = threshold\n",
    "                        clf.feature_idx = feature_i\n",
    "                        min_error = error\n",
    "\n",
    "            # calculate alpha\n",
    "            EPS = 1e-10\n",
    "            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
    "\n",
    "            # calculate predictions and update weights\n",
    "            predictions = clf.predict(X)\n",
    "\n",
    "            w *= np.exp(-clf.alpha * y * predictions)\n",
    "            # Normalize to one\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # Save classifier\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, X):\n",
    "        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
    "        y_pred = np.sum(clf_preds, axis=0)\n",
    "        y_pred = np.sign(y_pred)\n",
    "\n",
    "        return y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "clf = Adaboost(n_estimators = 10)\n",
    "clf.fit(train_df, y_train_df)\n",
    "y_pred = clf.predict(test_df)\n",
    "acc = 1 - accuracy_score(y_test_df, y_pred)\n",
    "print(f\"n_estimators: 10, Accuracy = {acc}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_estimators: 10, Accuracy = 0.8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "clf = Adaboost(n_estimators = 100)\n",
    "clf.fit(train_df, y_train_df)\n",
    "y_pred = clf.predict(test_df)\n",
    "acc = 1 - accuracy_score(y_test_df, y_pred)\n",
    "print(f\"n_estimators: 100, Accuracy = {acc}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_estimators: 100, Accuracy = 0.8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features,\n",
    "                 bootstrap=True, criterion='gini', max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        # If the max_features is not integer\n",
    "        # I round it\n",
    "        self.max_features = round(max_features)\n",
    "        self.bootstrap = bootstrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.tree_list = []\n",
    "\n",
    "    # Build the random forest\n",
    "    def build(self, X, y):\n",
    "        # If bootstrap is true, the data may be reused\n",
    "        if self.bootstrap is True:\n",
    "            for b in range(self.n_estimators):\n",
    "                # I random chose max_features features every time\n",
    "                X_ = X[:, random.sample(range(X.shape[1]),\n",
    "                                        self.max_features)]\n",
    "                # I random chose 120 datas to build decision tree\n",
    "                random_choose = random.sample(range(X.shape[0]), 120)\n",
    "                X_ = X[random_choose, :]\n",
    "                y_ = y[random_choose]\n",
    "                clf = DecisionTree(self.criterion, self.max_depth)\n",
    "                clf.fit(X_, y_)\n",
    "                self.tree_list.append(clf)  # Record the decision tree\n",
    "        else:  # If bootstrap is not true, the data will not be reused\n",
    "            random_train_list = np.arange(X.shape[1])\n",
    "            random.shuffle(random_train_list)\n",
    "            # I divide datas into n_estimators parts\n",
    "            n = round(X.shape[0] / self.n_estimators)\n",
    "            for b in range(self.n_estimators - 1):\n",
    "                # I random chose max_features feature every time\n",
    "                X_ = X[:, random.sample(range(X.shape[1]),\n",
    "                                        self.max_features)]\n",
    "                X_ = X[b*n:(b+1)*n, :]\n",
    "                y_ = y[b*n:(b+1)*n]\n",
    "                clf = DecisionTree(self.criterion, self.max_depth)\n",
    "                clf.fit(X_, y_)\n",
    "                self.tree_list.append(clf)\n",
    "            X_ = X[(self.n_estimators-1)*n:, :]\n",
    "            y_ = y[(self.n_estimators-1)*n:]\n",
    "            clf = DecisionTree(self.criterion, self.max_depth)\n",
    "            clf.fit(X_, y_)\n",
    "            self.tree_list.append(clf)  # Record the decision tree\n",
    "\n",
    "    def predict(self, test):\n",
    "        result_vote = np.zeros(test.shape[0])\n",
    "        for b in range(self.n_estimators):\n",
    "            predict = self.tree_list[b].predict(test)\n",
    "            result_vote = result_vote + np.array(predict)\n",
    "        # Calculate the probability of class\n",
    "        # Because there are two classes for in this assignment\n",
    "        # If probability >= 0.5, it belongs to class 1\n",
    "        # else it belongs to class 0\n",
    "        result_vote = result_vote / self.n_estimators\n",
    "        for i in range(test.shape[0]):\n",
    "            if result_vote[i] < 0.5:\n",
    "                result_vote[i] = 0\n",
    "            else:\n",
    "                result_vote[i] = 1\n",
    "        return result_vote\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_10tree.build(train_df, y_train_df)\n",
    "predict_10tree = clf_10tree.predict(test_df)\n",
    "acc_10tree = accuracy_score(predict_10tree, y_test_df)\n",
    "print(f\"criterion: gini, estimators = 10, bootstrap = True.\\n\\\n",
    "Accuracy of test-set = {acc_10tree}\")\n",
    "\n",
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]), \n",
    "                          bootstrap=True, criterion='gini', max_depth=None)\n",
    "clf_10tree.build(train_df, y_train_df)\n",
    "predict_10tree = clf_10tree.predict(test_df)\n",
    "acc_10tree = accuracy_score(predict_10tree, y_test_df)\n",
    "print(f\"criterion: gini, estimators = 10, bootstrap = False.\\n\\\n",
    "Accuracy of test-set = {acc_10tree}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: divide by zero encountered in log2\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n",
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "criterion: gini, estimators = 10, bootstrap = True.\n",
      "Accuracy of test-set = 0.74\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: divide by zero encountered in log2\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n",
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "criterion: gini, estimators = 10, bootstrap = False.\n",
      "Accuracy of test-set = 0.78\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_100tree.build(train_df, y_train_df)\n",
    "predict_100tree = clf_100tree.predict(test_df)\n",
    "acc_100tree = accuracy_score(predict_100tree, y_test_df)\n",
    "print(f\"criterion: gini, estimators = 100, bootstrap = True.\\n\\\n",
    "Accuracy of test-set = {acc_100tree}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: divide by zero encountered in log2\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n",
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "criterion: gini, estimators = 100, bootstrap = True.\n",
      "Accuracy of test-set = 0.8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of test data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(train_df.shape[1]))\n",
    "clf_random_features.build(train_df, y_train_df)\n",
    "predict_random_features = clf_random_features.predict(test_df)\n",
    "acc_random_features = accuracy_score(predict_random_features, y_test_df)\n",
    "print(f\"criterion: gini, estimators = 10 with random features.\\n\\\n",
    "Accuracy of test-set = {acc_random_features}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: divide by zero encountered in log2\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n",
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "criterion: gini, estimators = 10 with random features.\n",
      "Accuracy of test-set = 0.74\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "clf_all_features = RandomForest(n_estimators=10, max_features=train_df.shape[1])\n",
    "clf_all_features.build(train_df, y_train_df)\n",
    "predict_all_features = clf_all_features.predict(test_df)\n",
    "acc_all_features = accuracy_score(predict_all_features, y_test_df)\n",
    "print(f\"criterion: gini, estimators = 10 with all features.\\n\\\n",
    "Accuracy of test-set = {acc_all_features}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: divide by zero encountered in log2\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n",
      "/tmp/ipykernel_19376/3692151198.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -(p_0 * np.log2(p_0) + p_1 * np.log2(p_1))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "criterion: gini, estimators = 10 with all features.\n",
      "Accuracy of test-set = 0.74\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 6.\n",
    "Try you best to get highest test accuracy score by \n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you cannot call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "import time\n",
    "\n",
    "try:\n",
    "    # For python2\n",
    "    from itertools import izip as zip\n",
    "    LARGE_NUMBER = sys.maxint\n",
    "except ImportError:\n",
    "    # For python3\n",
    "    LARGE_NUMBER = sys.maxsize\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "class TreeNode(object):\n",
    "    def __init__(self):\n",
    "        self.is_leaf = False\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.split_feature_id = None\n",
    "        self.split_val = None\n",
    "        self.weight = None\n",
    "\n",
    "    def _calc_split_gain(self, G, H, G_l, H_l, G_r, H_r, lambd):\n",
    "        \"\"\"\n",
    "        Loss reduction\n",
    "        (Refer to Eq7 of Reference[1])\n",
    "        \"\"\"\n",
    "        def calc_term(g, h):\n",
    "            return np.square(g) / (h + lambd)\n",
    "        return calc_term(G_l, H_l) + calc_term(G_r, H_r) - calc_term(G, H)\n",
    "\n",
    "    def _calc_leaf_weight(self, grad, hessian, lambd):\n",
    "        \"\"\"\n",
    "        Calculate the optimal weight of this leaf node.\n",
    "        (Refer to Eq5 of Reference[1])\n",
    "        \"\"\"\n",
    "        return np.sum(grad) / (np.sum(hessian) + lambd)\n",
    "\n",
    "    def build(self, instances, grad, hessian, shrinkage_rate, depth, param):\n",
    "        \"\"\"\n",
    "        Exact Greedy Alogirithm for Split Finidng\n",
    "        (Refer to Algorithm1 of Reference[1])\n",
    "        \"\"\"\n",
    "        assert instances.shape[0] == len(grad) == len(hessian)\n",
    "        if depth > param['max_depth']:\n",
    "            self.is_leaf = True\n",
    "            self.weight = self._calc_leaf_weight(grad, hessian, param['lambda']) * shrinkage_rate\n",
    "            return\n",
    "        G = np.sum(grad)\n",
    "        H = np.sum(hessian)\n",
    "        best_gain = 0.\n",
    "        best_feature_id = None\n",
    "        best_val = 0.\n",
    "        best_left_instance_ids = None\n",
    "        best_right_instance_ids = None\n",
    "        for feature_id in range(instances.shape[1]):\n",
    "            G_l, H_l = 0., 0.\n",
    "            sorted_instance_ids = instances[:,feature_id].argsort()\n",
    "            for j in range(sorted_instance_ids.shape[0]):\n",
    "                G_l += grad[sorted_instance_ids[j]]\n",
    "                H_l += hessian[sorted_instance_ids[j]]\n",
    "                G_r = G - G_l\n",
    "                H_r = H - H_l\n",
    "                current_gain = self._calc_split_gain(G, H, G_l, H_l, G_r, H_r, param['lambda'])\n",
    "                if current_gain > best_gain:\n",
    "                    best_gain = current_gain\n",
    "                    best_feature_id = feature_id\n",
    "                    best_val = instances[sorted_instance_ids[j]][feature_id]\n",
    "                    best_left_instance_ids = sorted_instance_ids[:j+1]\n",
    "                    best_right_instance_ids = sorted_instance_ids[j+1:]\n",
    "        if best_gain < param['min_split_gain']:\n",
    "            self.is_leaf = True\n",
    "            self.weight = self._calc_leaf_weight(grad, hessian, param['lambda']) * shrinkage_rate\n",
    "        else:\n",
    "            self.split_feature_id = best_feature_id\n",
    "            self.split_val = best_val\n",
    "\n",
    "            self.left_child = TreeNode()\n",
    "            self.left_child.build(instances[best_left_instance_ids],\n",
    "                                  grad[best_left_instance_ids],\n",
    "                                  hessian[best_left_instance_ids],\n",
    "                                  shrinkage_rate,\n",
    "                                  depth+1, param)\n",
    "\n",
    "            self.right_child = TreeNode()\n",
    "            self.right_child.build(instances[best_right_instance_ids],\n",
    "                                   grad[best_right_instance_ids],\n",
    "                                   hessian[best_right_instance_ids],\n",
    "                                   shrinkage_rate,\n",
    "                                   depth+1, param)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.is_leaf:\n",
    "            return self.weight\n",
    "        else:\n",
    "            if x[self.split_feature_id] <= self.split_val:\n",
    "                return self.left_child.predict(x)\n",
    "            else:\n",
    "                return self.right_child.predict(x)\n",
    "\n",
    "\n",
    "class Tree(object):\n",
    "    ''' Classification and regression tree for tree ensemble '''\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def build(self, instances, grad, hessian, shrinkage_rate, param):\n",
    "        assert len(instances) == len(grad) == len(hessian)\n",
    "        self.root = TreeNode()\n",
    "        current_depth = 0\n",
    "        self.root.build(instances, grad, hessian, shrinkage_rate, current_depth, param)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.root.predict(x)\n",
    "\n",
    "\n",
    "class GBT(object):\n",
    "    def __init__(self):\n",
    "        self.params = {'gamma': 0.,\n",
    "                       'lambda': 1.,\n",
    "                       'min_split_gain': 0.1,\n",
    "                       'max_depth': 5,\n",
    "                       'learning_rate': 0.3,\n",
    "                       }\n",
    "        self.best_iteration = None\n",
    "\n",
    "    def _calc_training_data_scores(self, train_set, models):\n",
    "        if len(models) == 0:\n",
    "            return None\n",
    "        X = train_set.X\n",
    "        scores = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            scores[i] = self.predict(X[i], models=models)\n",
    "        return scores\n",
    "\n",
    "    def _calc_l2_gradient(self, train_set, scores):\n",
    "        labels = train_set.y\n",
    "        hessian = np.full(len(labels), 2)\n",
    "        if scores is None:\n",
    "            grad = np.random.uniform(size=len(labels))\n",
    "        else:\n",
    "            grad = np.array([2 * (labels[i] - scores[i]) for i in range(len(labels))])\n",
    "        return grad, hessian\n",
    "\n",
    "    def _calc_gradient(self, train_set, scores):\n",
    "        \"\"\"For now, only L2 loss is supported\"\"\"\n",
    "        return self._calc_l2_gradient(train_set, scores)\n",
    "\n",
    "    def _calc_l2_loss(self, models, data_set):\n",
    "        errors = []\n",
    "        for x, y in zip(data_set.X, data_set.y):\n",
    "            errors.append(y - self.predict(x, models))\n",
    "        return np.mean(np.square(errors))\n",
    "\n",
    "    def _calc_loss(self, models, data_set):\n",
    "        \"\"\"For now, only L2 loss is supported\"\"\"\n",
    "        return self._calc_l2_loss(models, data_set)\n",
    "\n",
    "    def _build_learner(self, train_set, grad, hessian, shrinkage_rate):\n",
    "        learner = Tree()\n",
    "        learner.build(train_set.X, grad, hessian, shrinkage_rate, self.params)\n",
    "        return learner\n",
    "\n",
    "    def train(self, params, train_set, num_boost_round=20, valid_set=None, early_stopping_rounds=5):\n",
    "        self.params.update(params)\n",
    "        models = []\n",
    "        shrinkage_rate = 1.\n",
    "        best_iteration = None\n",
    "        best_val_loss = LARGE_NUMBER\n",
    "        train_start_time = time.time()\n",
    "\n",
    "        print(\"Training until validation scores don't improve for {} rounds.\"\n",
    "              .format(early_stopping_rounds))\n",
    "        for iter_cnt in range(num_boost_round):\n",
    "            iter_start_time = time.time()\n",
    "            scores = self._calc_training_data_scores(train_set, models)\n",
    "            grad, hessian = self._calc_gradient(train_set, scores)\n",
    "            learner = self._build_learner(train_set, grad, hessian, shrinkage_rate)\n",
    "            if iter_cnt > 0:\n",
    "                shrinkage_rate *= self.params['learning_rate']\n",
    "            models.append(learner)\n",
    "            train_loss = self._calc_loss(models, train_set)\n",
    "            val_loss = self._calc_loss(models, valid_set) if valid_set else None\n",
    "            val_loss_str = '{:.10f}'.format(val_loss) if val_loss else '-'\n",
    "            print(\"Iter {:>3}, Train's L2: {:.10f}, Valid's L2: {}, Elapsed: {:.2f} secs\"\n",
    "                  .format(iter_cnt, train_loss, val_loss_str, time.time() - iter_start_time))\n",
    "            if val_loss is not None and val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_iteration = iter_cnt\n",
    "            if iter_cnt - best_iteration >= early_stopping_rounds:\n",
    "                print(\"Early stopping, best iteration is:\")\n",
    "                print(\"Iter {:>3}, Train's L2: {:.10f}\".format(best_iteration, best_val_loss))\n",
    "                break\n",
    "\n",
    "        self.models = models\n",
    "        self.best_iteration = best_iteration\n",
    "        print(\"Training finished. Elapsed: {:.2f} secs\".format(time.time() - train_start_time))\n",
    "\n",
    "    def predict(self, x, models=None, num_iteration=None):\n",
    "        if models is None:\n",
    "            models = self.models\n",
    "        assert models is not None\n",
    "        return np.sum(m.predict(x) for m in models[:num_iteration])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "train_data = Dataset(train_df, y_train_df)\n",
    "eval_data = Dataset(test_df, y_test_df)\n",
    "\n",
    "params = {}\n",
    "\n",
    "#print('Start training...')\n",
    "gbt = GBT()\n",
    "gbt.train(params,\n",
    "          train_data,\n",
    "          num_boost_round=20,\n",
    "          valid_set=eval_data,\n",
    "          early_stopping_rounds=5)\n",
    "\n",
    "#print('Start predicting...')\n",
    "y_pred = []\n",
    "for x in test_df:\n",
    "    y_pred.append(gbt.predict(x, num_iteration=gbt.best_iteration))\n",
    "\n",
    "print('(gradient boosting) Test-set accuarcy score:', accuracy_score(y_test_df, y_pred) ** 0.5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 5 rounds.\n",
      "Iter   0, Train's L2: 0.2096462939, Valid's L2: 0.1594599166, Elapsed: 0.16 secs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_19376/145482852.py:208: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(m.predict(x) for m in models[:num_iteration])\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iter   1, Train's L2: 0.2897199446, Valid's L2: 0.3457068213, Elapsed: 0.19 secs\n",
      "Iter   2, Train's L2: 0.1931976385, Valid's L2: 0.2628902820, Elapsed: 0.17 secs\n",
      "Iter   3, Train's L2: 0.1822841583, Valid's L2: 0.2501923336, Elapsed: 0.17 secs\n",
      "Iter   4, Train's L2: 0.1793127418, Valid's L2: 0.2476501821, Elapsed: 0.17 secs\n",
      "Iter   5, Train's L2: 0.1786028018, Valid's L2: 0.2472484510, Elapsed: 0.17 secs\n",
      "Early stopping, best iteration is:\n",
      "Iter   0, Train's L2: 0.1594599166\n",
      "Training finished. Elapsed: 1.04 secs\n",
      "(gradient boosting) Test-set accuarcy score: 0.8944271909999159\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}